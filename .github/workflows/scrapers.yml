name: Daily Scrapers

on:
  schedule:
    # Run daily at 6 AM and 6 PM UTC (covers Copenhagen peak hours)
    - cron: '0 6,18 * * *'
  workflow_dispatch: # Allow manual trigger

env:
  PYTHON_VERSION: "3.11"

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r data-collection/scrapers/social_scrapers/requirements.txt

    - name: Create data directory and initialize database
      run: |
        mkdir -p data
        # Create a simple database initialization script
        python -c "
        import duckdb
        import os
        from datetime import datetime
        
        # Create/connect to database
        conn = duckdb.connect('./data/events.duckdb')
        
        # FIRST: Drop all existing tables to start clean
        print('🔍 Cleaning existing database...')
        try:
            tables = conn.execute('SHOW TABLES').fetchall()
            print(f'Found existing tables: {tables}')
            
            # Drop tables in dependency order
            conn.execute('DROP TABLE IF EXISTS recommendation_logs')
            conn.execute('DROP TABLE IF EXISTS interactions') 
            conn.execute('DROP TABLE IF EXISTS event_artists')
            conn.execute('DROP TABLE IF EXISTS events')
            conn.execute('DROP TABLE IF EXISTS active_events')
            conn.execute('DROP TABLE IF EXISTS artists')
            conn.execute('DROP TABLE IF EXISTS venues')
            conn.execute('DROP TABLE IF EXISTS users')
            print('✅ Dropped all existing tables')
        except Exception as e:
            print(f'Error during cleanup: {e}')
        
        # Create tables matching EXACT schema expected by main.py API queries
        # Based on actual SQL queries in main.py lines 130-217
        
        conn.execute('''
        CREATE TABLE IF NOT EXISTS venues (
            id TEXT PRIMARY KEY,
            name TEXT,
            address TEXT,
            neighborhood TEXT
        )
        ''')
        
        conn.execute('''
        CREATE TABLE IF NOT EXISTS events (
            id TEXT PRIMARY KEY,
            title TEXT,
            description TEXT,
            date_time TIMESTAMP,
            end_date_time TIMESTAMP,
            price_min REAL,
            price_max REAL,
            currency TEXT DEFAULT 'DKK',
            venue_id TEXT,
            source TEXT,
            source_url TEXT,
            image_url TEXT,
            popularity_score REAL DEFAULT 0.0,
            status TEXT DEFAULT 'active'
        )
        ''')
        
        # Add a test venue first
        conn.execute('''
        INSERT OR REPLACE INTO venues (id, name, address, neighborhood)
        VALUES (?, ?, ?, ?)
        ''', ['test-venue-1', 'Test Venue', 'Copenhagen, Denmark', 'City Center'])
        
        # Add a test event (using date_time as expected by main.py)
        conn.execute('''
        INSERT OR REPLACE INTO events (id, title, description, date_time, venue_id, source, status)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', ['test-event-1', 'Database Test Event', 'Test event created during deployment', datetime.now(), 'test-venue-1', 'system', 'active'])
        
        conn.commit()
        conn.close()
        print('✅ Database initialized successfully')
        "
        
    - name: Download existing database (if exists)
      env:
        RAILWAY_DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
      run: |
        # Download the current database from Railway if it exists
        # This step assumes you'll set up a mechanism to download the DB
        echo "Database sync would happen here"

    - name: Run event scrapers
      env:
        DATABASE_URL: ./data/events.duckdb
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "Running event scrapers..."
        cd data-collection/scrapers
        python runner.py --source all --max-events 1000
      continue-on-error: true

    - name: Run social scrapers
      env:
        DATABASE_URL: ./data/events.duckdb
        PYTHONPATH: ${{ github.workspace }}
        # Add social media API keys as secrets
        INSTAGRAM_USERNAME: ${{ secrets.INSTAGRAM_USERNAME }}
        INSTAGRAM_PASSWORD: ${{ secrets.INSTAGRAM_PASSWORD }}
      run: |
        echo "Running social media scrapers..."
        cd data-collection/scrapers/social_scrapers
        python run_all_scrapers.py
      continue-on-error: true

    - name: Validate database
      env:
        DATABASE_URL: ./data/events.duckdb
      run: |
        python -c "
        import duckdb
        conn = duckdb.connect('./data/events.duckdb')
        count = conn.execute('SELECT COUNT(*) FROM events WHERE date_time > CURRENT_DATE').fetchone()[0]
        print(f'Found {count} upcoming events')
        conn.close()
        "

    - name: Upload database artifact
      uses: actions/upload-artifact@v4
      with:
        name: events-database-${{ github.run_number }}
        path: data/events.duckdb
        retention-days: 7

    - name: Sync database to Railway
      env:
        RAILWAY_DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
      run: |
        # Upload the updated database back to Railway
        echo "Database sync back to Railway would happen here"
        # You'll need to implement this based on Railway's file storage options
        
    - name: Create scraping report
      run: |
        echo "## Scraping Report - $(date)" >> scraping_report.md
        echo "- Run ID: ${{ github.run_number }}" >> scraping_report.md
        echo "- Timestamp: $(date)" >> scraping_report.md
        
        # Add database stats
        python -c "
        import duckdb
        from datetime import datetime
        conn = duckdb.connect('./data/events.duckdb')
        
        total = conn.execute('SELECT COUNT(*) FROM events').fetchone()[0]
        upcoming = conn.execute('SELECT COUNT(*) FROM events WHERE date_time > CURRENT_TIMESTAMP').fetchone()[0]
        today = conn.execute('SELECT COUNT(*) FROM events WHERE DATE(date_time) = CURRENT_DATE').fetchone()[0]
        
        print(f'- Total events: {total}')
        print(f'- Upcoming events: {upcoming}')
        print(f'- Events today: {today}')
        conn.close()
        " >> scraping_report.md
        
        cat scraping_report.md

    - name: Comment on commit (if push)
      if: github.event_name == 'push'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('scraping_report.md', 'utf8');
          
          github.rest.repos.createCommitComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            commit_sha: context.sha,
            body: report
          });