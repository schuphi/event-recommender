name: Daily Scrapers

on:
  schedule:
    # Run daily at 6 AM and 6 PM UTC (covers Copenhagen peak hours)
    - cron: '0 6,18 * * *'
  workflow_dispatch: # Allow manual trigger

env:
  PYTHON_VERSION: "3.11"

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r data-collection/scrapers/social_scrapers/requirements.txt

    - name: Initialize database with existing complex schema
      run: |
        mkdir -p data
        # Remove any existing database file to start clean
        rm -f ./data/events.duckdb
        
        # Create database with the same schema as database/schema.sql but simplified
        python -c "
        import duckdb
        from datetime import datetime
        import uuid
        
        conn = duckdb.connect('./data/events.duckdb')
        
        # Create venues table with exact schema that exists in Railway
        conn.execute('''
        CREATE TABLE venues (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            address TEXT,
            lat REAL NOT NULL,
            lon REAL NOT NULL,
            h3_index TEXT NOT NULL,
            neighborhood TEXT,
            venue_type TEXT,
            capacity INTEGER,
            website TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Create events table with schema matching Railway
        conn.execute('''
        CREATE TABLE events (
            id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            description TEXT,
            date_time TIMESTAMP NOT NULL,
            end_date_time TIMESTAMP,
            price_min REAL,
            price_max REAL,
            currency TEXT DEFAULT 'DKK',
            venue_id TEXT NOT NULL,
            artist_ids TEXT,
            source TEXT NOT NULL,
            source_id TEXT,
            source_url TEXT,
            image_url TEXT,
            h3_index TEXT,
            embedding BLOB,
            content_features TEXT,
            popularity_score REAL DEFAULT 0.0,
            status TEXT DEFAULT 'active',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Insert a test venue with all required fields including lat/lon
        venue_id = str(uuid.uuid4())
        conn.execute('''
        INSERT INTO venues (id, name, address, lat, lon, h3_index, neighborhood, venue_type, capacity)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', [venue_id, 'Test Venue', 'Copenhagen, Denmark', 55.6761, 12.5683, 'mock_h3_index', 'City Center', 'club', 500])
        
        # Insert a test event
        event_id = str(uuid.uuid4())
        conn.execute('''
        INSERT INTO events (id, title, description, date_time, venue_id, source, h3_index, status)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', [event_id, 'Database Test Event', 'Test event created during deployment', datetime.now(), venue_id, 'system', 'mock_h3_index', 'active'])
        
        # Debug: Check what columns actually exist in events table
        print('🔍 Checking events table schema:')
        schema = conn.execute('DESCRIBE events').fetchall()
        print(f'Events table columns: {schema}')
        
        # Create active_events view after events table is created (only if status column exists)
        try:
            conn.execute('CREATE VIEW active_events AS SELECT * FROM events WHERE status = \"active\"')
            print('✅ Created active_events view')
        except Exception as e:
            print(f'⚠️  Could not create active_events view: {e}')
            print('Creating simple view without status filter')
            conn.execute('CREATE VIEW active_events AS SELECT * FROM events')
        
        conn.commit()
        conn.close()
        print('✅ Database initialized with complex schema')
        "
        
    - name: Download existing database (if exists)
      env:
        RAILWAY_DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
      run: |
        # Download the current database from Railway if it exists
        # This step assumes you'll set up a mechanism to download the DB
        echo "Database sync would happen here"

    - name: Run event scrapers
      env:
        DATABASE_URL: ./data/events.duckdb
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "Running event scrapers..."
        cd data-collection/scrapers
        python runner.py --source all --max-events 1000
      continue-on-error: true

    - name: Run social scrapers
      env:
        DATABASE_URL: ./data/events.duckdb
        PYTHONPATH: ${{ github.workspace }}
        # Add social media API keys as secrets
        INSTAGRAM_USERNAME: ${{ secrets.INSTAGRAM_USERNAME }}
        INSTAGRAM_PASSWORD: ${{ secrets.INSTAGRAM_PASSWORD }}
      run: |
        echo "Running social media scrapers..."
        cd data-collection/scrapers/social_scrapers
        python run_all_scrapers.py
      continue-on-error: true

    - name: Validate database
      env:
        DATABASE_URL: ./data/events.duckdb
      run: |
        python -c "
        import duckdb
        conn = duckdb.connect('./data/events.duckdb')
        count = conn.execute('SELECT COUNT(*) FROM events WHERE date_time > CURRENT_DATE').fetchone()[0]
        print(f'Found {count} upcoming events')
        conn.close()
        "

    - name: Upload database artifact
      uses: actions/upload-artifact@v4
      with:
        name: events-database-${{ github.run_number }}
        path: data/events.duckdb
        retention-days: 7

    - name: Sync database to Railway
      env:
        RAILWAY_DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
      run: |
        # Upload the updated database back to Railway
        echo "Database sync back to Railway would happen here"
        # You'll need to implement this based on Railway's file storage options
        
    - name: Create scraping report
      run: |
        echo "## Scraping Report - $(date)" >> scraping_report.md
        echo "- Run ID: ${{ github.run_number }}" >> scraping_report.md
        echo "- Timestamp: $(date)" >> scraping_report.md
        
        # Add database stats
        python -c "
        import duckdb
        from datetime import datetime
        conn = duckdb.connect('./data/events.duckdb')
        
        total = conn.execute('SELECT COUNT(*) FROM events').fetchone()[0]
        upcoming = conn.execute('SELECT COUNT(*) FROM events WHERE date_time > CURRENT_TIMESTAMP').fetchone()[0]
        today = conn.execute('SELECT COUNT(*) FROM events WHERE DATE(date_time) = CURRENT_DATE').fetchone()[0]
        
        print(f'- Total events: {total}')
        print(f'- Upcoming events: {upcoming}')
        print(f'- Events today: {today}')
        conn.close()
        " >> scraping_report.md
        
        cat scraping_report.md

    - name: Comment on commit (if push)
      if: github.event_name == 'push'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('scraping_report.md', 'utf8');
          
          github.rest.repos.createCommitComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            commit_sha: context.sha,
            body: report
          });